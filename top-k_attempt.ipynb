{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (0.20.1)\n",
      "Requirement already satisfied: fastai in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (2.7.18)\n",
      "Requirement already satisfied: matplotlib in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (3.9.2)\n",
      "Requirement already satisfied: filelock in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: fsspec in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: numpy in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: fastcore<1.8,>=1.5.29 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (1.7.19)\n",
      "Requirement already satisfied: requests in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (2.32.3)\n",
      "Requirement already satisfied: pandas in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (2.2.3)\n",
      "Requirement already satisfied: fastdownload<2,>=0.0.5 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (0.0.7)\n",
      "Requirement already satisfied: packaging in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (24.2)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (1.0.3)\n",
      "Requirement already satisfied: scipy in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (1.14.1)\n",
      "Requirement already satisfied: spacy<4 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (3.8.2)\n",
      "Requirement already satisfied: pip in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (23.0.1)\n",
      "Requirement already satisfied: pyyaml in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (6.0.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (1.5.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (1.0.10)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (0.13.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (2.4.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (3.0.12)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (3.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (2.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (2.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (8.3.2)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (65.5.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (3.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (4.67.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (2.9.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->fastai) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->fastai) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->fastai) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->fastai) (3.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas->fastai) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas->fastai) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from scikit-learn->fastai) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from scikit-learn->fastai) (1.4.2)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<4->fastai) (1.2.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (2.23.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (0.7.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.0->spacy<4->fastai) (0.1.5)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.0->spacy<4->fastai) (1.0.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (1.5.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (8.1.7)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai) (7.0.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4->fastai) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4->fastai) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: opencv-python in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from opencv-python) (2.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: fastai in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (2.7.18)\n",
      "Requirement already satisfied: torchvision in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (0.20.1)\n",
      "Requirement already satisfied: pyyaml in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (6.0.2)\n",
      "Requirement already satisfied: scipy in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (1.14.1)\n",
      "Requirement already satisfied: matplotlib in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (3.9.2)\n",
      "Requirement already satisfied: pip in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (23.0.1)\n",
      "Requirement already satisfied: pandas in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (2.2.3)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (1.0.3)\n",
      "Requirement already satisfied: packaging in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (24.2)\n",
      "Requirement already satisfied: fastcore<1.8,>=1.5.29 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (1.7.19)\n",
      "Requirement already satisfied: scikit-learn in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (1.5.2)\n",
      "Requirement already satisfied: torch<2.6,>=1.10 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (2.5.1)\n",
      "Requirement already satisfied: requests in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (2.32.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (11.0.0)\n",
      "Requirement already satisfied: spacy<4 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (3.8.2)\n",
      "Requirement already satisfied: fastdownload<2,>=0.0.5 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fastai) (0.0.7)\n",
      "Requirement already satisfied: numpy in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: networkx in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch<2.6,>=1.10->fastai) (3.4.2)\n",
      "Requirement already satisfied: fsspec in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch<2.6,>=1.10->fastai) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch<2.6,>=1.10->fastai) (1.13.1)\n",
      "Requirement already satisfied: jinja2 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch<2.6,>=1.10->fastai) (3.1.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch<2.6,>=1.10->fastai) (4.12.2)\n",
      "Requirement already satisfied: filelock in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch<2.6,>=1.10->fastai) (3.16.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from sympy==1.13.1->torch<2.6,>=1.10->fastai) (1.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (1.0.10)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (2.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (3.0.9)\n",
      "Requirement already satisfied: setuptools in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (65.5.0)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (8.3.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (1.0.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (3.4.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (3.0.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (1.1.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (2.9.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (4.67.0)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (0.4.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (2.4.8)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy<4->fastai) (0.13.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->fastai) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->fastai) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->fastai) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->fastai) (3.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib->fastai) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib->fastai) (2.9.0.post0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib->fastai) (3.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib->fastai) (1.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib->fastai) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib->fastai) (4.54.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas->fastai) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas->fastai) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from scikit-learn->fastai) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from scikit-learn->fastai) (1.4.2)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<4->fastai) (1.2.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (2.23.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->fastai) (1.16.0)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.0->spacy<4->fastai) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.0->spacy<4->fastai) (0.1.5)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (13.9.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (1.5.4)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai) (7.0.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai) (0.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from jinja2->torch<2.6,>=1.10->fastai) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4->fastai) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4->fastai) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/sammizhu/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision fastai matplotlib\n",
    "!pip install opencv-python\n",
    "!pip install --upgrade fastai torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from fastai.vision.all import untar_data, URLs\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagenette path: /Users/sammizhu/.fastai/data/imagenette2\n",
      "Imagewoof path: /Users/sammizhu/.fastai/data/imagewoof2\n"
     ]
    }
   ],
   "source": [
    "imagenette_path = untar_data(URLs.IMAGENETTE)\n",
    "imagewoof_path = untar_data(URLs.IMAGEWOOF)\n",
    "\n",
    "print(f\"Imagenette path: {imagenette_path}\")\n",
    "print(f\"Imagewoof path: {imagewoof_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for training and validation\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create Datasets\n",
    "imagenette_train_dataset = datasets.ImageFolder(imagenette_path/'train', transform=train_transforms)\n",
    "imagenette_val_dataset = datasets.ImageFolder(imagenette_path/'val', transform=val_transforms)\n",
    "\n",
    "imagewoof_train_dataset = datasets.ImageFolder(imagewoof_path/'train', transform=train_transforms)\n",
    "imagewoof_val_dataset = datasets.ImageFolder(imagewoof_path/'val', transform=val_transforms)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "imagenette_train_loader = DataLoader(imagenette_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "imagenette_val_loader = DataLoader(imagenette_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "imagewoof_train_loader = DataLoader(imagewoof_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "imagewoof_val_loader = DataLoader(imagewoof_val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model (ResNet18 for example)\n",
    "def create_model(num_classes):\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    # Replace the final fully connected layer with the correct number of classes\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "# Imagenette and Imagewoof both have 10 classes\n",
    "imagenette_model = create_model(num_classes=10)\n",
    "imagewoof_model = create_model(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (Adam or SGD)\n",
    "imagenette_optimizer = optim.Adam(imagenette_model.parameters(), lr=0.001)\n",
    "imagewoof_optimizer = optim.Adam(imagewoof_model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    model = model.to(device)\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        # Compute average loss for this epoch\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # Validate the model\n",
    "        val_acc = evaluate_model(model, val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    print(f\"Training complete. Best validation accuracy: {best_acc:.4f}\")\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            # Update correct predictions\n",
    "            correct += torch.sum(preds == labels).item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Imagenette dataset...\n",
      "Training on Imagewoof dataset...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train the model on Imagenette\n",
    "print(\"Training on Imagenette dataset...\")\n",
    "# train_model(imagenette_model, imagenette_train_loader, imagenette_val_loader, criterion, imagenette_optimizer, num_epochs=1)\n",
    "\n",
    "# Train the model on Imagewoof\n",
    "print(\"Training on Imagewoof dataset...\")\n",
    "# train_model(imagewoof_model, imagewoof_train_loader, imagewoof_val_loader, criterion, imagewoof_optimizer, num_epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_c/2_16kg0x5s5_54m_2cvq27500000gn/T/ipykernel_20361/1334256160.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  imagenette_model.load_state_dict(torch.load('models/imagenette_best_model.pth'))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/imagenette_best_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save the best model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# torch.save(imagenette_model.state_dict(), 'imagenette_best_model.pth')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# torch.save(imagewoof_model.state_dict(), 'imagewoof_best_model.pth')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the models for inference\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m imagenette_model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/imagenette_best_model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m imagewoof_model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/imagewoof_best_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/imagenette_best_model.pth'"
     ]
    }
   ],
   "source": [
    "# Save the best model\n",
    "# torch.save(imagenette_model.state_dict(), 'imagenette_best_model.pth')\n",
    "# torch.save(imagewoof_model.state_dict(), 'imagewoof_best_model.pth')\n",
    "\n",
    "# Load the models for inference\n",
    "imagenette_model.load_state_dict(torch.load('models/imagenette_best_model.pth'))\n",
    "imagewoof_model.load_state_dict(torch.load('models/imagewoof_best_model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your trained model (e.g., on Imagewoof)\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_classes = 10 \n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# model.load_state_dict(torch.load('imagewoof_best_model.pth'))\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store activations for multiple layers\n",
    "activations = {}\n",
    "\n",
    "# Hook function to save activations from any layer\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output\n",
    "    return hook\n",
    "\n",
    "# Register hooks on all layers that are convolutional\n",
    "for name, layer in model.named_modules():\n",
    "    if isinstance(layer, torch.nn.Conv2d):  # Only capture Conv2d layers\n",
    "        layer.register_forward_hook(get_activation(name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/sammizhu/cs2822r-project/to_use/test_img/dog.jpeg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m input_tensor, image\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load the image of your dog\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m input_tensor, image \u001b[38;5;241m=\u001b[39m \u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_img/dog.jpeg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 11\u001b[0m, in \u001b[0;36mload_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_image\u001b[39m(image_path):\n\u001b[0;32m---> 11\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m preprocess(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m input_tensor, image\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/PIL/Image.py:3469\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3466\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3469\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3470\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/sammizhu/cs2822r-project/to_use/test_img/dog.jpeg'"
     ]
    }
   ],
   "source": [
    "# Define image preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load and preprocess the dog image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = preprocess(image).unsqueeze(0)\n",
    "    return input_tensor, image\n",
    "\n",
    "# Load the image of your dog\n",
    "input_tensor, image = load_image('test_img/dog.jpeg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Move the input tensor to the device\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "# Forward pass to get the model's output\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Get the predicted class\n",
    "target_class = output.argmax().item()\n",
    "\n",
    "# Zero out any previous gradients\n",
    "model.zero_grad()\n",
    "\n",
    "# Backward pass to compute gradients for the target class\n",
    "output[:, target_class].backward(retain_graph=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the Grad-CAM heatmap\n",
    "def grad_cam(activation, gradients):\n",
    "    # Global average pooling over the gradients (average the gradients per feature map)\n",
    "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "    # Multiply each channel in the activation map by the pooled gradients\n",
    "    for i in range(activation.size(1)):\n",
    "        activation[:, i, :, :] *= pooled_gradients[i]\n",
    "    # Create the heatmap by averaging the weighted activation maps\n",
    "    heatmap = torch.mean(activation, dim=1).squeeze()\n",
    "    # Apply ReLU to remove negative values and normalize the heatmap\n",
    "    heatmap = torch.relu(heatmap)\n",
    "    heatmap -= heatmap.min()\n",
    "    heatmap /= heatmap.max()\n",
    "    return heatmap.detach().cpu().numpy()\n",
    "\n",
    "# Function to save the heatmap without overlaying on the original image\n",
    "def save_heatmap_only(heatmap, layer_name, i, save_folder=\"all_layers\", colormap='jet'):\n",
    "    # Ensure save folder exists\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    # Normalize heatmap to 8-bit and resize\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap_resized = cv2.resize(heatmap, (1229, 1229), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # Apply colormap\n",
    "    heatmap_colored = cv2.applyColorMap(heatmap_resized, cv2.COLORMAP_JET)\n",
    "    \n",
    "    # Save the heatmap as an image file\n",
    "    save_path = os.path.join(save_folder, f\"cnn_layer_{i}.png\")\n",
    "    cv2.imwrite(save_path, heatmap_colored)\n",
    "    print(f\"Saved Grad-CAM heatmap for layer '{layer_name}' to: {save_path}\")\n",
    "\n",
    "# Main function to generate Grad-CAM for all layers and save each heatmap\n",
    "def generate_gradcam_for_all_layers(activations, target_class, output, i=1):\n",
    "    for layer_name, activation in activations.items():\n",
    "        try:\n",
    "            # Compute gradients with allow_unused=True\n",
    "            gradients = torch.autograd.grad(\n",
    "                outputs=output[:, target_class], \n",
    "                inputs=activation, \n",
    "                retain_graph=True, \n",
    "                allow_unused=True\n",
    "            )[0]\n",
    "\n",
    "            if gradients is None:\n",
    "                print(f\"No gradients found for layer: {layer_name}\")\n",
    "                continue\n",
    "\n",
    "            # Generate the Grad-CAM heatmap\n",
    "            heatmap = grad_cam(activation, gradients)\n",
    "            print(f\"Generating and saving Grad-CAM heatmap for layer: {layer_name}\")\n",
    "\n",
    "            # Save the heatmap without overlaying on the original image\n",
    "            save_heatmap_only(heatmap, layer_name, i)\n",
    "            i += 1  # Increment the layer counter for each saved image\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error processing layer {layer_name}: {str(e)}\")\n",
    "\n",
    "# Example usage\n",
    "# Ensure `activations`, `target_class`, and `output` are defined\n",
    "generate_gradcam_for_all_layers(activations, target_class, output, i=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of contours (regions) found: 46\n",
      "Including contour with intensity: 131553.10426540283\n",
      "Including contour with intensity: 42222.44549763037\n",
      "Including contour with intensity: 22034.12796208531\n",
      "Including contour with intensity: 21112.587677725118\n",
      "Including contour with intensity: 20697.606635071097\n",
      "Including contour with intensity: 17722.061611374407\n",
      "Including contour with intensity: 15256.668246445499\n",
      "Including contour with intensity: 15154.175355450237\n",
      "Including contour with intensity: 12437.691943127962\n",
      "Including contour with intensity: 7948.857819905213\n",
      "Including contour with intensity: 6995.127962085309\n",
      "Including contour with intensity: 5126.009478672986\n",
      "Including contour with intensity: 4751.582938388626\n",
      "Including contour with intensity: 2953.4786729857824\n",
      "Including contour with intensity: 2837.805687203792\n",
      "Including contour with intensity: 2281.303317535545\n",
      "Including contour with intensity: 2104.7962085308063\n",
      "Including contour with intensity: 1916.0616113744074\n",
      "Saved image with Top-18 distinct Grad-CAM regions for 'cnn_layer_6' to: top_k_regions/rose_cnn_layer_6_top18.png\n",
      "Number of contours (regions) found: 33\n",
      "Including contour with intensity: 59614.4123222749\n",
      "Including contour with intensity: 37247.91469194312\n",
      "Including contour with intensity: 29633.535545023704\n",
      "Including contour with intensity: 26631.246445497625\n",
      "Including contour with intensity: 20408.85781990522\n",
      "Including contour with intensity: 19636.41706161137\n",
      "Including contour with intensity: 10084.606635071088\n",
      "Including contour with intensity: 8695.786729857822\n",
      "Including contour with intensity: 5665.469194312796\n",
      "Including contour with intensity: 5507.905213270143\n",
      "Including contour with intensity: 4677.587677725118\n",
      "Including contour with intensity: 4161.554502369669\n",
      "Including contour with intensity: 3858.194312796209\n",
      "Including contour with intensity: 3191.289099526067\n",
      "Including contour with intensity: 1404.1184834123223\n",
      "Including contour with intensity: 1057.2938388625594\n",
      "Including contour with intensity: 969.4502369668248\n",
      "Including contour with intensity: 949.6919431279622\n",
      "Saved image with Top-18 distinct Grad-CAM regions for 'cnn_layer_7' to: top_k_regions/rose_cnn_layer_7_top18.png\n",
      "Number of contours (regions) found: 52\n",
      "Including contour with intensity: 12408.203791469194\n",
      "Including contour with intensity: 10652.345971563982\n",
      "Including contour with intensity: 10486.924170616116\n",
      "Including contour with intensity: 4504.1706161137445\n",
      "Including contour with intensity: 3807.654028436019\n",
      "Including contour with intensity: 3121.123222748815\n",
      "Including contour with intensity: 2856.5829383886257\n",
      "Including contour with intensity: 2520.1327014218014\n",
      "Including contour with intensity: 2289.317535545024\n",
      "Including contour with intensity: 2140.3649289099526\n",
      "Including contour with intensity: 1587.5781990521327\n",
      "Including contour with intensity: 1405.829383886256\n",
      "Including contour with intensity: 1262.061611374408\n",
      "Including contour with intensity: 895.3080568720379\n",
      "Including contour with intensity: 740.0236966824644\n",
      "Including contour with intensity: 637.6824644549763\n",
      "Including contour with intensity: 500.7488151658768\n",
      "Including contour with intensity: 438.18009478672985\n",
      "Saved image with Top-18 distinct Grad-CAM regions for 'cnn_layer_5' to: top_k_regions/rose_cnn_layer_5_top18.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def save_top_k_regions_on_gradcam(grad_cam, output_filename, top_k=1, save_folder=\"top_k_regions\"):\n",
    "    \"\"\"\n",
    "    Saves an image showing only the top-K most intense distinct regions from a Grad-CAM heatmap using contour-based detection.\n",
    "\n",
    "    Parameters:\n",
    "    grad_cam (numpy array): The Grad-CAM heatmap (2D array, values between 0-1).\n",
    "    output_filename (str): Base filename for the saved image.\n",
    "    top_k (int): The number of top distinct regions to highlight based on intensity.\n",
    "    save_folder (str): Folder to save the output images.\n",
    "    \"\"\"\n",
    "    # Step 1: Normalize Grad-CAM to range [0, 1] if not already normalized\n",
    "    grad_cam -= grad_cam.min()\n",
    "    if grad_cam.max() > 0:\n",
    "        grad_cam /= grad_cam.max()\n",
    "\n",
    "    # Step 2: Convert Grad-CAM to 8-bit for contour detection\n",
    "    grad_cam_8bit = np.uint8(255 * grad_cam)\n",
    "\n",
    "    # Step 3: Apply thresholding to better separate regions\n",
    "    _, binary_map = cv2.threshold(grad_cam_8bit, 100, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Step 4: Find contours representing distinct regions\n",
    "    contours, _ = cv2.findContours(binary_map, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    print(f\"Number of contours (regions) found: {len(contours)}\")\n",
    "\n",
    "    # Step 5: Calculate intensity for each contour region using the original grad_cam values\n",
    "    contour_intensities = []\n",
    "    for i, contour in enumerate(contours):\n",
    "        mask = np.zeros_like(grad_cam, dtype=np.uint8)\n",
    "        cv2.drawContours(mask, [contour], -1, 1, thickness=-1)  # Fill contour area with 1s\n",
    "        intensity = np.sum(grad_cam * mask)  # Sum intensity within the contour\n",
    "        contour_intensities.append((contour, intensity))\n",
    "\n",
    "    # Step 6: Sort contours by intensity and select the top-k contours\n",
    "    top_k_contours = sorted(contour_intensities, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "    # Step 7: Create a mask to display only the top-k contours\n",
    "    top_k_mask = np.zeros_like(grad_cam)\n",
    "    for contour, intensity in top_k_contours:\n",
    "        print(f\"Including contour with intensity: {intensity}\")\n",
    "        cv2.drawContours(top_k_mask, [contour], -1, 1, thickness=-1)\n",
    "\n",
    "    # Step 8: Apply the mask on the Grad-CAM to isolate and display only the top-k regions\n",
    "    top_k_grad_cam = grad_cam * top_k_mask\n",
    "    top_k_grad_cam_colored = cv2.applyColorMap(np.uint8(255 * top_k_grad_cam), cv2.COLORMAP_JET)\n",
    "\n",
    "    # Ensure the save folder exists\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    # Define the save path with the original image name and k value\n",
    "    save_path = os.path.join(save_folder, f\"rose_{output_filename}_top{top_k}.png\")\n",
    "    cv2.imwrite(save_path, top_k_grad_cam_colored)\n",
    "    print(f\"Saved image with Top-{top_k} distinct Grad-CAM regions for '{output_filename}' to: {save_path}\")\n",
    "\n",
    "def process_folder_of_gradcams(gradcam_folder, top_k=1, save_folder=\"top_k_regions\"):\n",
    "    \"\"\"\n",
    "    Processes a folder of Grad-CAM heatmaps and saves only the top-K distinct regions for each image.\n",
    "\n",
    "    Parameters:\n",
    "    gradcam_folder (str): Path to the folder containing the Grad-CAM heatmap images.\n",
    "    top_k (int): The number of top distinct regions to highlight based on intensity.\n",
    "    save_folder (str): Folder to save the output images.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(gradcam_folder):\n",
    "        gradcam_path = os.path.join(gradcam_folder, filename)\n",
    "        \n",
    "        # Load the Grad-CAM heatmap\n",
    "        grad_cam = cv2.imread(gradcam_path, cv2.IMREAD_GRAYSCALE) / 255.0  # Normalize the Grad-CAM\n",
    "        \n",
    "        # Check that the Grad-CAM heatmap loaded correctly\n",
    "        if grad_cam is None:\n",
    "            print(f\"Error loading {filename}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Save the Top-K distinct regions from the Grad-CAM heatmap\n",
    "        output_filename = os.path.splitext(filename)[0]  # Use filename without extension\n",
    "        save_top_k_regions_on_gradcam(grad_cam, output_filename, top_k, save_folder)\n",
    "\n",
    "# Example Usage:\n",
    "gradcam_folder = \"/Users/sammizhu/cs2822r-project/rose\"  # Folder containing Grad-CAM heatmap variations\n",
    "process_folder_of_gradcams(gradcam_folder, top_k=18)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
